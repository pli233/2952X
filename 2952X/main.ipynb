{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means 簇数量: 1000\n",
      "每个数据点的标签: [ 66  66  66 ... 932  50 503]\n",
      "每个簇的大小: {0: 1989, 1: 1100, 2: 1195, 3: 1585, 4: 1181, 5: 762, 6: 1118, 7: 1420, 8: 847, 9: 1124, 10: 1253, 11: 1542, 12: 1090, 13: 1220, 14: 1005, 15: 1409, 16: 1524, 17: 1269, 18: 1413, 19: 1542, 20: 1768, 21: 1886, 22: 1298, 23: 1322, 24: 1383, 25: 1512, 26: 1453, 27: 854, 28: 1726, 29: 800, 30: 1515, 31: 1284, 32: 2082, 33: 638, 34: 1181, 35: 727, 36: 1646, 37: 979, 38: 732, 39: 956, 40: 551, 41: 1442, 42: 1628, 43: 1132, 44: 1122, 45: 898, 46: 1193, 47: 1054, 48: 825, 49: 1571, 50: 2586, 51: 1238, 52: 1472, 53: 1773, 54: 1291, 55: 1275, 56: 1437, 57: 1093, 58: 1232, 59: 1274, 60: 1939, 61: 1582, 62: 1401, 63: 923, 64: 1032, 65: 1258, 66: 1174, 67: 1291, 68: 1453, 69: 1249, 70: 2168, 71: 1769, 72: 690, 73: 2151, 74: 666, 75: 1494, 76: 988, 77: 2275, 78: 1865, 79: 1232, 80: 1453, 81: 3230, 82: 1949, 83: 1099, 84: 879, 85: 1270, 86: 1282, 87: 1909, 88: 1333, 89: 2046, 90: 893, 91: 1334, 92: 1198, 93: 1062, 94: 1784, 95: 1850, 96: 957, 97: 2209, 98: 1333, 99: 1298, 100: 992, 101: 868, 102: 1997, 103: 832, 104: 1294, 105: 1320, 106: 1957, 107: 1394, 108: 1627, 109: 1868, 110: 1362, 111: 2411, 112: 1368, 113: 2486, 114: 1539, 115: 818, 116: 1218, 117: 1213, 118: 1211, 119: 1700, 120: 1603, 121: 676, 122: 1228, 123: 980, 124: 1236, 125: 1373, 126: 630, 127: 1834, 128: 1117, 129: 1137, 130: 1833, 131: 935, 132: 2545, 133: 1165, 134: 1275, 135: 1626, 136: 1261, 137: 1155, 138: 2017, 139: 1506, 140: 1249, 141: 1075, 142: 964, 143: 800, 144: 1983, 145: 1144, 146: 1203, 147: 2308, 148: 1253, 149: 1287, 150: 731, 151: 1390, 152: 2622, 153: 3104, 154: 1165, 155: 2008, 156: 1092, 157: 2257, 158: 1244, 159: 1134, 160: 1247, 161: 1033, 162: 1253, 163: 1990, 164: 1436, 165: 1306, 166: 772, 167: 1016, 168: 860, 169: 1550, 170: 1547, 171: 998, 172: 910, 173: 1105, 174: 1351, 175: 1279, 176: 2751, 177: 1299, 178: 1649, 179: 1100, 180: 1236, 181: 1739, 182: 970, 183: 1211, 184: 1543, 185: 1318, 186: 1635, 187: 1005, 188: 807, 189: 949, 190: 1149, 191: 682, 192: 1252, 193: 1507, 194: 2433, 195: 1712, 196: 1215, 197: 1299, 198: 2702, 199: 1839, 200: 1251, 201: 1308, 202: 1962, 203: 843, 204: 1253, 205: 1103, 206: 2556, 207: 1351, 208: 1078, 209: 462, 210: 1112, 211: 1314, 212: 1714, 213: 1482, 214: 1710, 215: 1634, 216: 1246, 217: 1039, 218: 1086, 219: 1477, 220: 1142, 221: 1633, 222: 822, 223: 1250, 224: 1248, 225: 1252, 226: 831, 227: 721, 228: 1149, 229: 397, 230: 1572, 231: 869, 232: 1081, 233: 1163, 234: 2441, 235: 2439, 236: 1479, 237: 909, 238: 1187, 239: 1640, 240: 1007, 241: 1132, 242: 1236, 243: 1192, 244: 1213, 245: 1503, 246: 1142, 247: 1132, 248: 2527, 249: 1269, 250: 1196, 251: 1241, 252: 1433, 253: 1266, 254: 1697, 255: 1311, 256: 834, 257: 1068, 258: 1044, 259: 1309, 260: 1260, 261: 1373, 262: 1143, 263: 345, 264: 1539, 265: 1138, 266: 1095, 267: 1040, 268: 1266, 269: 1554, 270: 1399, 271: 1127, 272: 901, 273: 1598, 274: 407, 275: 2208, 276: 1340, 277: 1344, 278: 1285, 279: 992, 280: 1371, 281: 1468, 282: 1098, 283: 1877, 284: 1360, 285: 1012, 286: 928, 287: 1197, 288: 1214, 289: 1280, 290: 2610, 291: 1223, 292: 1043, 293: 2288, 294: 1922, 295: 1094, 296: 1533, 297: 2504, 298: 1338, 299: 1251, 300: 1234, 301: 2718, 302: 1085, 303: 1057, 304: 1115, 305: 1134, 306: 1016, 307: 1475, 308: 888, 309: 1442, 310: 1544, 311: 1214, 312: 1473, 313: 1386, 314: 1286, 315: 1102, 316: 1775, 317: 1044, 318: 1157, 319: 1035, 320: 1392, 321: 1407, 322: 1920, 323: 1937, 324: 1282, 325: 1325, 326: 1148, 327: 1248, 328: 1217, 329: 1246, 330: 1284, 331: 2194, 332: 1175, 333: 1331, 334: 1268, 335: 1049, 336: 1218, 337: 1773, 338: 1309, 339: 1089, 340: 1561, 341: 1633, 342: 1321, 343: 865, 344: 1243, 345: 1312, 346: 1961, 347: 1945, 348: 918, 349: 1352, 350: 1408, 351: 861, 352: 959, 353: 1677, 354: 975, 355: 1214, 356: 1093, 357: 875, 358: 1568, 359: 1175, 360: 1106, 361: 1321, 362: 1168, 363: 1475, 364: 1261, 365: 1259, 366: 1521, 367: 1246, 368: 979, 369: 1308, 370: 2023, 371: 1103, 372: 1063, 373: 1473, 374: 1292, 375: 2115, 376: 1227, 377: 1277, 378: 1018, 379: 1404, 380: 587, 381: 1404, 382: 812, 383: 1184, 384: 813, 385: 1398, 386: 1478, 387: 2158, 388: 1604, 389: 1269, 390: 1210, 391: 1261, 392: 572, 393: 1417, 394: 1127, 395: 1447, 396: 1277, 397: 1927, 398: 1288, 399: 1171, 400: 1391, 401: 1133, 402: 1563, 403: 1272, 404: 1516, 405: 1352, 406: 1405, 407: 2080, 408: 1705, 409: 1251, 410: 1712, 411: 1632, 412: 491, 413: 800, 414: 1097, 415: 1024, 416: 766, 417: 1137, 418: 1230, 419: 1289, 420: 2248, 421: 731, 422: 2570, 423: 1232, 424: 928, 425: 1393, 426: 1379, 427: 970, 428: 522, 429: 886, 430: 926, 431: 1365, 432: 612, 433: 1094, 434: 1770, 435: 1672, 436: 1198, 437: 990, 438: 813, 439: 1245, 440: 1020, 441: 1266, 442: 677, 443: 1444, 444: 1329, 445: 502, 446: 1114, 447: 1464, 448: 1585, 449: 1020, 450: 647, 451: 1293, 452: 1092, 453: 1209, 454: 1643, 455: 1223, 456: 1253, 457: 1168, 458: 1824, 459: 1656, 460: 1255, 461: 643, 462: 1028, 463: 2587, 464: 1070, 465: 1271, 466: 1297, 467: 1011, 468: 807, 469: 1204, 470: 1818, 471: 1919, 472: 969, 473: 1263, 474: 1161, 475: 603, 476: 1458, 477: 1326, 478: 1233, 479: 1788, 480: 800, 481: 1130, 482: 1160, 483: 981, 484: 1176, 485: 1649, 486: 1328, 487: 1320, 488: 1243, 489: 1064, 490: 941, 491: 1292, 492: 1252, 493: 1222, 494: 1083, 495: 752, 496: 1219, 497: 1090, 498: 1113, 499: 969, 500: 728, 501: 1226, 502: 1202, 503: 616, 504: 1385, 505: 1357, 506: 847, 507: 1190, 508: 2065, 509: 1135, 510: 1375, 511: 1078, 512: 1298, 513: 950, 514: 1429, 515: 1235, 516: 1004, 517: 1225, 518: 1519, 519: 1793, 520: 1369, 521: 1246, 522: 1215, 523: 1478, 524: 1144, 525: 2120, 526: 977, 527: 1692, 528: 2168, 529: 1243, 530: 1106, 531: 1332, 532: 591, 533: 1497, 534: 1662, 535: 1706, 536: 596, 537: 971, 538: 1152, 539: 736, 540: 842, 541: 1045, 542: 1245, 543: 3176, 544: 1126, 545: 1078, 546: 1214, 547: 1176, 548: 975, 549: 1110, 550: 572, 551: 3617, 552: 1188, 553: 937, 554: 1838, 555: 1807, 556: 1175, 557: 1264, 558: 1953, 559: 656, 560: 1271, 561: 885, 562: 1803, 563: 819, 564: 1203, 565: 1165, 566: 2035, 567: 1178, 568: 2555, 569: 1118, 570: 1685, 571: 1382, 572: 1107, 573: 1169, 574: 1194, 575: 1281, 576: 1123, 577: 1912, 578: 722, 579: 1273, 580: 1075, 581: 1266, 582: 914, 583: 1487, 584: 1780, 585: 1088, 586: 621, 587: 1311, 588: 1025, 589: 911, 590: 1193, 591: 1002, 592: 1515, 593: 1688, 594: 1147, 595: 1097, 596: 792, 597: 930, 598: 953, 599: 763, 600: 792, 601: 875, 602: 787, 603: 1179, 604: 1689, 605: 575, 606: 1418, 607: 835, 608: 972, 609: 1303, 610: 918, 611: 1243, 612: 1033, 613: 1876, 614: 976, 615: 1375, 616: 1412, 617: 1438, 618: 744, 619: 821, 620: 1179, 621: 1423, 622: 1280, 623: 1230, 624: 1214, 625: 1222, 626: 2403, 627: 1901, 628: 718, 629: 936, 630: 1002, 631: 1767, 632: 790, 633: 1018, 634: 666, 635: 1086, 636: 1855, 637: 1519, 638: 1321, 639: 583, 640: 1503, 641: 695, 642: 764, 643: 1302, 644: 858, 645: 1060, 646: 1286, 647: 2405, 648: 959, 649: 835, 650: 824, 651: 798, 652: 1089, 653: 2090, 654: 1360, 655: 1083, 656: 1364, 657: 1026, 658: 1287, 659: 1896, 660: 983, 661: 894, 662: 1707, 663: 1968, 664: 864, 665: 1284, 666: 1226, 667: 1398, 668: 1683, 669: 1224, 670: 1164, 671: 1324, 672: 1262, 673: 1586, 674: 943, 675: 1477, 676: 1182, 677: 2122, 678: 627, 679: 1583, 680: 920, 681: 698, 682: 1087, 683: 1198, 684: 1294, 685: 1153, 686: 1170, 687: 846, 688: 947, 689: 730, 690: 1391, 691: 1076, 692: 1108, 693: 1081, 694: 1451, 695: 2344, 696: 1425, 697: 1121, 698: 1531, 699: 1439, 700: 802, 701: 1414, 702: 984, 703: 1296, 704: 826, 705: 1114, 706: 1260, 707: 744, 708: 1388, 709: 1099, 710: 1200, 711: 1041, 712: 1308, 713: 782, 714: 644, 715: 1491, 716: 1284, 717: 1100, 718: 1373, 719: 1934, 720: 1828, 721: 633, 722: 1158, 723: 516, 724: 733, 725: 858, 726: 2553, 727: 1754, 728: 1319, 729: 1464, 730: 1247, 731: 1168, 732: 1588, 733: 1835, 734: 1456, 735: 635, 736: 1089, 737: 375, 738: 1211, 739: 1669, 740: 1132, 741: 999, 742: 782, 743: 1648, 744: 1838, 745: 1011, 746: 1231, 747: 752, 748: 1351, 749: 1286, 750: 928, 751: 1849, 752: 2379, 753: 1360, 754: 808, 755: 2026, 756: 1128, 757: 1952, 758: 1268, 759: 1651, 760: 999, 761: 1001, 762: 1296, 763: 718, 764: 1879, 765: 761, 766: 863, 767: 1921, 768: 730, 769: 1339, 770: 1235, 771: 1254, 772: 1110, 773: 940, 774: 1521, 775: 1020, 776: 1263, 777: 1107, 778: 1959, 779: 927, 780: 536, 781: 1376, 782: 1036, 783: 1239, 784: 1273, 785: 1071, 786: 1016, 787: 1776, 788: 1491, 789: 1218, 790: 2347, 791: 780, 792: 1539, 793: 1472, 794: 1448, 795: 1152, 796: 1683, 797: 1293, 798: 3072, 799: 806, 800: 2543, 801: 892, 802: 1671, 803: 1040, 804: 1102, 805: 739, 806: 1355, 807: 1039, 808: 1130, 809: 1949, 810: 726, 811: 2712, 812: 1734, 813: 660, 814: 1236, 815: 1111, 816: 496, 817: 779, 818: 1180, 819: 2164, 820: 1177, 821: 1310, 822: 723, 823: 1259, 824: 694, 825: 1028, 826: 1164, 827: 1121, 828: 972, 829: 1128, 830: 1738, 831: 1300, 832: 1211, 833: 718, 834: 665, 835: 555, 836: 1106, 837: 876, 838: 1070, 839: 631, 840: 1126, 841: 1425, 842: 922, 843: 1057, 844: 1670, 845: 929, 846: 2088, 847: 1068, 848: 1428, 849: 828, 850: 2471, 851: 1674, 852: 1127, 853: 1366, 854: 1343, 855: 1206, 856: 1235, 857: 851, 858: 1175, 859: 1885, 860: 1019, 861: 1030, 862: 648, 863: 1113, 864: 1903, 865: 1053, 866: 970, 867: 1182, 868: 1207, 869: 1097, 870: 867, 871: 919, 872: 1226, 873: 1914, 874: 1619, 875: 1267, 876: 815, 877: 1099, 878: 725, 879: 973, 880: 1248, 881: 1850, 882: 1313, 883: 1392, 884: 1379, 885: 901, 886: 961, 887: 1738, 888: 1285, 889: 1098, 890: 1744, 891: 632, 892: 1025, 893: 969, 894: 551, 895: 1187, 896: 1329, 897: 1277, 898: 1495, 899: 867, 900: 2023, 901: 428, 902: 814, 903: 1080, 904: 675, 905: 1236, 906: 643, 907: 1228, 908: 1498, 909: 1192, 910: 1510, 911: 1488, 912: 836, 913: 1288, 914: 447, 915: 1046, 916: 1047, 917: 693, 918: 1347, 919: 1019, 920: 1749, 921: 1297, 922: 1271, 923: 741, 924: 1081, 925: 932, 926: 1725, 927: 1305, 928: 2683, 929: 1075, 930: 878, 931: 1106, 932: 798, 933: 1360, 934: 792, 935: 968, 936: 1285, 937: 988, 938: 1328, 939: 1160, 940: 1698, 941: 633, 942: 3090, 943: 2415, 944: 871, 945: 1037, 946: 1275, 947: 2006, 948: 848, 949: 1093, 950: 865, 951: 1183, 952: 1465, 953: 531, 954: 1203, 955: 1539, 956: 906, 957: 807, 958: 1479, 959: 1373, 960: 994, 961: 1503, 962: 561, 963: 1254, 964: 846, 965: 816, 966: 1790, 967: 732, 968: 990, 969: 1479, 970: 620, 971: 1252, 972: 1931, 973: 1185, 974: 920, 975: 940, 976: 1026, 977: 1645, 978: 1023, 979: 1261, 980: 817, 981: 2608, 982: 1228, 983: 864, 984: 818, 985: 849, 986: 1285, 987: 1650, 988: 642, 989: 1274, 990: 564, 991: 844, 992: 1531, 993: 1259, 994: 1517, 995: 749, 996: 686, 997: 775, 998: 2129, 999: 1142}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "from cuml.cluster import KMeans  # 使用 cuML 的 KMeans 实现\n",
    "\n",
    "def density_cluster_kmeans(n_clusters=10):\n",
    "    # 加载特征矩阵\n",
    "    feature_matrix = np.load('/CSCI2952X/imagenet_features.npy')\n",
    "    feature_matrix = feature_matrix.astype(np.float32)\n",
    "    \n",
    "    # 将特征矩阵转换为 GPU 上的格式\n",
    "    feature_matrix_gpu = cp.asarray(feature_matrix)  # 使用 CuPy 将 NumPy 数组转换为 GPU 数组\n",
    "    \n",
    "    # 使用 GPU 加速的 KMeans\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=1000)  # 参数可根据需要调整\n",
    "    labels = kmeans.fit_predict(feature_matrix_gpu)\n",
    "\n",
    "    # 获取簇中心并将其保存为文件\n",
    "    cluster_centers = kmeans.cluster_centers_.get()  # 将簇中心从 GPU 转移回 CPU\n",
    "    np.save('cluster_centers.npy', cluster_centers)  # 保存为 cluster_centers.npy 文件\n",
    "\n",
    "    # 将 GPU 上的标签数组转换回 CPU\n",
    "    labels = labels.get()\n",
    "\n",
    "    # 打印聚类结果\n",
    "    print(f'K-Means 簇数量: {n_clusters}')\n",
    "    print(f'每个数据点的标签: {labels}')\n",
    "\n",
    "    # 统计每个簇的大小\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    cluster_sizes = dict(zip(unique, counts))\n",
    "    print(f'每个簇的大小: {cluster_sizes}')\n",
    "\n",
    "# 运行替换后的聚类函数\n",
    "density_cluster_kmeans(n_clusters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp  \n",
    "from cuml.cluster import DBSCAN  # 使用 cuML 的 DBSCAN 实现\n",
    "\n",
    "def density_cluster():\n",
    "    feature_matrix = np.load('/CSCI2952X/imagenet_features.npy')\n",
    "    feature_matrix = feature_matrix.astype(np.float32)\n",
    "    \n",
    "    # 将特征矩阵转换为 GPU 上的格式\n",
    "    feature_matrix_gpu = cp.asarray(feature_matrix)  # 使用 CuPy 将 NumPy 数组转换为 GPU 数组\n",
    "    \n",
    "    # 使用 GPU 加速的 DBSCAN\n",
    "    dbscan = DBSCAN(eps=1, min_samples=100)  # 可根据需要调整参数\n",
    "    labels = dbscan.fit_predict(feature_matrix_gpu)\n",
    "\n",
    "    # 将 GPU 上的标签数组转换回 CPU\n",
    "    labels = labels.get()\n",
    "\n",
    "    # 打印聚类结果\n",
    "    num_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    print(f'发现的簇数量: {num_clusters}')\n",
    "    print(f'每个数据点的标签: {labels}')\n",
    "\n",
    "    # 统计每个簇的大小\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    cluster_sizes = dict(zip(unique, counts))\n",
    "    print(f'每个簇的大小: {cluster_sizes}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/peiyuan/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_LinearClassifierWrapper(\n",
       "  (backbone): DinoVisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x NestedTensorBlock(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MemEffAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (linear_head): Linear(in_features=3840, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14_reg_lc')\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:   0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_762749/1344307438.py:36: UserWarning: Using a target size (torch.Size([1000])) that is different to the input size (torch.Size([1, 1000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = torch.nn.functional.mse_loss(output, target_center)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 0, Loss: 12.391698837280273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:   5%|▌         | 1/20 [00:00<00:09,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 1, Loss: 8.085114479064941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:  10%|█         | 2/20 [00:01<00:10,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 2, Loss: 7.4083967208862305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:  15%|█▌        | 3/20 [00:01<00:10,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 3, Loss: 6.728322982788086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:  20%|██        | 4/20 [00:02<00:09,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 4, Loss: 6.108248233795166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:  25%|██▌       | 5/20 [00:02<00:09,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 5, Loss: 5.387615203857422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:  30%|███       | 6/20 [00:03<00:08,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 6, Loss: 4.602733612060547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:  35%|███▌      | 7/20 [00:04<00:07,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 7, Loss: 4.009591102600098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:  40%|████      | 8/20 [00:04<00:07,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 8, Loss: 3.5333447456359863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:  45%|████▌     | 9/20 [00:05<00:06,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 9, Loss: 3.2467446327209473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:  50%|█████     | 10/20 [00:06<00:06,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 10, Loss: 3.0989649295806885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:  55%|█████▌    | 11/20 [00:06<00:05,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 11, Loss: 2.907315254211426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:  60%|██████    | 12/20 [00:07<00:04,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 12, Loss: 2.697659730911255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:  65%|██████▌   | 13/20 [00:07<00:04,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 13, Loss: 2.5199391841888428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:  70%|███████   | 14/20 [00:08<00:03,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 14, Loss: 2.3732781410217285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:  75%|███████▌  | 15/20 [00:09<00:03,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 15, Loss: 2.236978769302368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:  80%|████████  | 16/20 [00:09<00:02,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 16, Loss: 2.0868077278137207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:  85%|████████▌ | 17/20 [00:10<00:01,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 17, Loss: 1.943324327468872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:  90%|█████████ | 18/20 [00:10<00:01,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 18, Loss: 1.8073451519012451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0:  95%|█████████▌| 19/20 [00:11<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Step 19, Loss: 1.6737275123596191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 0: 100%|██████████| 20/20 [00:12<00:00,  1.65it/s]\n",
      "Optimizing for cluster 1:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1, Step 0, Loss: 8.694229125976562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 1:   5%|▌         | 1/20 [00:00<00:11,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1, Step 1, Loss: 6.7000274658203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 1:  10%|█         | 2/20 [00:01<00:11,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1, Step 2, Loss: 6.140081405639648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 1:  15%|█▌        | 3/20 [00:01<00:10,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1, Step 3, Loss: 5.405917644500732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 1:  20%|██        | 4/20 [00:02<00:09,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1, Step 4, Loss: 4.533076286315918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 1:  25%|██▌       | 5/20 [00:03<00:09,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1, Step 5, Loss: 4.046082973480225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 1:  30%|███       | 6/20 [00:03<00:08,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1, Step 6, Loss: 3.6787939071655273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 1:  35%|███▌      | 7/20 [00:04<00:07,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1, Step 7, Loss: 3.3146657943725586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 1:  40%|████      | 8/20 [00:04<00:07,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1, Step 8, Loss: 3.0591447353363037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 1:  45%|████▌     | 9/20 [00:05<00:06,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1, Step 9, Loss: 2.923186779022217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 1:  50%|█████     | 10/20 [00:06<00:06,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1, Step 10, Loss: 2.6968085765838623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 1:  55%|█████▌    | 11/20 [00:06<00:05,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1, Step 11, Loss: 2.528254985809326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 1:  60%|██████    | 12/20 [00:07<00:04,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1, Step 12, Loss: 2.3534226417541504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 1:  65%|██████▌   | 13/20 [00:07<00:03,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1, Step 13, Loss: 2.1972827911376953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 1:  70%|███████   | 14/20 [00:08<00:03,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1, Step 14, Loss: 2.0445380210876465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 1:  75%|███████▌  | 15/20 [00:09<00:03,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1, Step 15, Loss: 1.8734688758850098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing for cluster 1:  80%|████████  | 16/20 [00:09<00:02,  1.63it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Normalize the input image and pass it through the model\u001b[39;00m\n\u001b[1;32m     32\u001b[0m normalized_image \u001b[38;5;241m=\u001b[39m normalize(input_image\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_image\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure 'model' is properly defined\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Compute the mean squared error loss with the target cluster center\u001b[39;00m\n\u001b[1;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(output, target_center)\n",
      "File \u001b[0;32m~/miniconda3/envs/scdd/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/scdd/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/hub/classifiers.py:71\u001b[0m, in \u001b[0;36m_LinearClassifierWrapper.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# fmt: on\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m---> 71\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_intermediate_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_class_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     linear_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\n\u001b[1;32m     74\u001b[0m         x[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     75\u001b[0m         x[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m         x[\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     79\u001b[0m     ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:309\u001b[0m, in \u001b[0;36mDinoVisionTransformer.get_intermediate_layers\u001b[0;34m(self, x, n, reshape, return_class_token, norm)\u001b[0m\n\u001b[1;32m    307\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_intermediate_layers_chunked(x, n)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_intermediate_layers_not_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    311\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(out) \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:278\u001b[0m, in \u001b[0;36mDinoVisionTransformer._get_intermediate_layers_not_chunked\u001b[0;34m(self, x, n)\u001b[0m\n\u001b[1;32m    276\u001b[0m blocks_to_take \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(total_block_len \u001b[38;5;241m-\u001b[39m n, total_block_len) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m n\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m--> 278\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m blocks_to_take:\n\u001b[1;32m    280\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/scdd/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/scdd/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:254\u001b[0m, in \u001b[0;36mNestedTensorBlock.forward\u001b[0;34m(self, x_or_x_list)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_or_x_list):\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x_or_x_list, Tensor):\n\u001b[0;32m--> 254\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_or_x_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x_or_x_list, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m XFORMERS_AVAILABLE:\n",
      "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:112\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(ffn_residual_func(x))  \u001b[38;5;66;03m# FIXME: drop_path2\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[43mattn_residual_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m ffn_residual_func(x)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:91\u001b[0m, in \u001b[0;36mBlock.forward.<locals>.attn_residual_func\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattn_residual_func\u001b[39m(x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/scdd/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/scdd/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:77\u001b[0m, in \u001b[0;36mMemEffAttention.forward\u001b[0;34m(self, x, attn_bias)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attn_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxFormers is required for using nested tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m B, N, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     80\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv(x)\u001b[38;5;241m.\u001b[39mreshape(B, N, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, C \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n",
      "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv(x)\u001b[38;5;241m.\u001b[39mreshape(B, N, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, C \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     60\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, qkv[\u001b[38;5;241m1\u001b[39m], qkv[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m---> 61\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     64\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_drop(attn)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load precomputed cluster centers\n",
    "cluster_centers = np.load('cluster_centers.npy')\n",
    "cluster_centers = torch.tensor(cluster_centers, dtype=torch.float32)\n",
    "\n",
    "# Define a function to clamp image values to [0, 1]\n",
    "def clamp_image(img):\n",
    "    return img.clamp(0, 1)\n",
    "\n",
    "# Normalization transform (for input to the model)\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Iterate over each cluster center\n",
    "for i, target_center in enumerate(cluster_centers):\n",
    "    # Initialize an input image with random values\n",
    "    input_image = torch.randn((1, 3, 224, 224), requires_grad=True)\n",
    "\n",
    "    # Set up an optimizer for the current image\n",
    "    optimizer = optim.Adam([input_image], lr=0.01)\n",
    "\n",
    "    # Gradient-based optimization loop\n",
    "    for step in tqdm(range(20), desc=f'Optimizing for cluster {i}'):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Normalize the input image and pass it through the model\n",
    "        normalized_image = normalize(input_image.squeeze(0)).unsqueeze(0)\n",
    "        output = model(normalized_image)  # Ensure 'model' is properly defined\n",
    "        \n",
    "        # Compute the mean squared error loss with the target cluster center\n",
    "        loss = torch.nn.functional.mse_loss(output, target_center)\n",
    "        print(f'Cluster {i}, Step {step}, Loss: {loss.item()}')\n",
    "        \n",
    "        # Backpropagate and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Clamp image values to keep them in the range [0, 1]\n",
    "        input_image.data = clamp_image(input_image.data)\n",
    "\n",
    "    # Save the generated image for the current cluster center\n",
    "    generated_image = input_image.detach().squeeze(0).permute(1, 2, 0).numpy()\n",
    "    generated_image = (generated_image * 255).astype(np.uint8)  # Convert to 0-255 range\n",
    "    Image.fromarray(generated_image).save(f'generated_image_cluster_{i}.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scdd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
